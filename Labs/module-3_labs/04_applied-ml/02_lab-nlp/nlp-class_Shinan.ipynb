{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\april\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\april\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\april\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
<<<<<<< HEAD
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
=======
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "metadata": {},
   "outputs": [],
   "source": [
    "kittycat = 'We are all agreeing with the cats on this one, and she is too!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Cleanup"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are all agreeing with the cats on this one and she is too\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "source": [
    "import re\n",
    "def clean_up(text):\n",
    "    import re\n",
    "    text = re.sub('[^A-Za-z0-9 ]','',text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "kittycat_clean = clean_up(kittycat)\n",
    "print(kittycat_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'all', 'agreeing', 'with', 'the', 'cats', 'on', 'this', 'one', 'and', 'she', 'is', 'too']\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "kittycat_tokenize = word_tokenize(kittycat_clean)\n",
    "print(kittycat_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'all', 'agreeing', 'with', 'the', 'cat', 'on', 'this', 'one', 'and', 'she', 'is', 'too']\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "source": [
    "kittycat_lemmatize = [lemmatizer.lemmatize(item) for item in kittycat_tokenize]\n",
    "print(kittycat_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'all', 'agre', 'with', 'the', 'cat', 'on', 'this', 'one', 'and', 'she', 'is', 'too']\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "source": [
    "kittycat_stem = [stemmer.stem(item) for item in kittycat_lemmatize]\n",
    "print(kittycat_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agre', 'cat', 'one']\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "kittycat_nostopwords = [item for item in kittycat_stem if not item in stopwords_list]\n",
    "print(kittycat_nostopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Text"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
<<<<<<< HEAD
    "# help(CountVectorizer)"
=======
    "help(CountVectorizer)"
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "source": [
    "vectorizer.fit_transform(kittycat_nostopwords).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying to real data: IMDB movie reviews\n",
    "\n",
    "Get the data from here: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "An example walkthrough: https://dropsofai.com/sentiment-analysis-with-python-bag-of-words/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Large Movie Review Dataset v1.0\n",
    "\n",
    "Overview\n",
    "\n",
    "This dataset contains movie reviews along with their associated binary\n",
    "sentiment polarity labels. It is intended to serve as a benchmark for\n",
    "sentiment classification. \n",
    "\n",
    "Dataset \n",
    "\n",
    "The core dataset contains 50,000 reviews split evenly into 25k train\n",
    "and 25k test sets. The overall distribution of labels is balanced (25k\n",
    "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
    "documents for unsupervised learning. \n",
    "\n",
    "In the labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
    "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
    "more neutral ratings are not included in the train/test sets. In the\n",
    "unsupervised set, reviews of any rating are included and there are an\n",
    "even number of reviews > 5 and <= 5.\n",
    "\n",
    "Files\n",
    "\n",
    "There are two top-level directories [train/, test/] corresponding to\n",
    "the training and test sets. Each contains [pos/, neg/] directories for\n",
    "the reviews with binary labels positive and negative. Within these\n",
    "directories, reviews are stored in text files named following the\n",
    "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
    "the star rating for that review on a 1-10 scale. For example, the file\n",
    "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
    "example with unique id 200 and star rating 8/10 from IMDb. The\n",
    "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
    "omitted for this portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# reading positive reviews\n",
    "txt_folder = Path('aclImdb/train/pos').rglob('*.txt')\n",
    "files = [x for x in txt_folder]\n",
    "content = []\n",
    "for name in files:\n",
    "    f = open(name, 'r')  \n",
    "    content.append(f.readlines()[0])\n",
    "    f.close()\n",
<<<<<<< HEAD
    "pos = pd.DataFrame(content)\n",
    "pos.head()"
=======
    "pos = pd.DataFrame(content)"
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": null,
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading negative reviews\n",
    "txt_folder = Path('aclImdb/train/neg').rglob('*.txt')\n",
    "files = [x for x in txt_folder]\n",
    "content = []\n",
    "for name in files:\n",
    "    f = open(name, 'r')  \n",
    "    content.append(f.readlines()[0])\n",
    "    f.close()\n",
    "neg = pd.DataFrame(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "# Filipa  2:16 PM\n",
    "# try to add encoding=\"utf8\" inside open(). like:\n",
    "# for name in files:\n",
    "#     f = open(name, 'r', encoding=\"utf8\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
    "# we will try to predict whether a review is positive\n",
    "pos['target'] = 1\n",
    "neg['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting both dataframes together\n",
    "df = pd.concat([pos, neg], axis = 0)\n",
    "df.rename(columns = {0:'review'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is very large, so we are only taking a subset for analysis\n",
    "df = df.sample(frac=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_clean'] = df['review'].apply(clean_up)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "df['review_tokenize'] = df['review_clean'].apply(word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['review_lemmatize'] = df['review_tokenize'].apply(lambda row: [lemmatizer.lemmatize(item) for item in row])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "df['review_stem'] = df['review_lemmatize'].apply(lambda row: [stemmer.stem(item) for item in row])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "df['review_nostopwords'] = df['review_stem'].apply(lambda row: [item for item in row if not item in stopwords_list])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df['review_nostopwords']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['target'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(precision_score(y_test, pred))\n",
    "print(recall_score(y_test, pred))\n",
    "print(f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# the 1 and 0 class are balanced \n",
    "# the current dataset is more balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.3"
=======
   "version": "3.7.7"
>>>>>>> 533059893d145c6747d695ad68539ce5baf7adab
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
